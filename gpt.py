import os
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from typing import List, Any
from uuid import uuid4

import requests
from pydantic import BaseModel, field_validator, model_validator
from openai import OpenAI
from dotenv import load_dotenv
from supabase import create_client, Client

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
supabase: Client = create_client(os.environ["SUPABASE_URL"], os.environ["SUPABASE_KEY"])

# Pydantic Schema - å½ˆæ€§ç‰ˆæœ¬
class SelectedHeadline(BaseModel):
    title: str
    reason: str
    writing_direction: str

class HeadlineSelection(BaseModel):
    selections: List[SelectedHeadline]
    
    @model_validator(mode='before')
    @classmethod
    def extract_selections(cls, data: Any) -> Any:
        """è‡ªå‹•å¾ä¸åŒçš„éµåä¸­æå–é¸é …é™£åˆ—"""
        if isinstance(data, dict):
            # æŒ‰å„ªå…ˆé †åºæª¢æŸ¥å¯èƒ½çš„éµå
            possible_keys = [
                'selections',
                'selected_articles', 
                'articles',
                'news',
                'selected_news',
                'items',
                'results'
            ]
            
            for key in possible_keys:
                if key in data and isinstance(data[key], list):
                    print(f"ğŸ”§ ä½¿ç”¨éµåï¼š{key}")
                    selections_data = data[key]
                    
                    # é©—è­‰é™£åˆ—ä¸ç‚ºç©º
                    if len(selections_data) == 0:
                        raise ValueError('é¸é …é™£åˆ—ä¸èƒ½ç‚ºç©º')
                    
                    # ç°¡å–®æ¸…ç†è³‡æ–™ï¼Œç¢ºä¿æ¯å€‹é …ç›®éƒ½æœ‰å¿…è¦çš„æ¬„ä½
                    cleaned_selections = []
                    for i, item in enumerate(selections_data):
                        if isinstance(item, dict):
                            cleaned_item = {
                                'title': item.get('title', f'æ–°è {i+1}'),
                                'reason': item.get('reason', 'æœªæä¾›ç†ç”±'),
                                'writing_direction': item.get('writing_direction', 'æœªæä¾›å»ºè­°')
                            }
                            cleaned_selections.append(cleaned_item)
                    
                    print(f"âœ… æˆåŠŸè™•ç† {len(cleaned_selections)} å‰‡æ–°è")
                    return {'selections': cleaned_selections}
            
            # å¦‚æœæ‰¾ä¸åˆ°é æœŸçš„éµï¼Œå˜—è©¦æ‰¾ä»»ä½•é™£åˆ—
            for key, value in data.items():
                if isinstance(value, list) and len(value) > 0:
                    # æª¢æŸ¥é™£åˆ—ç¬¬ä¸€å€‹å…ƒç´ æ˜¯å¦çœ‹èµ·ä¾†åƒæ–°èé …ç›®
                    first_item = value[0]
                    if isinstance(first_item, dict) and any(k in first_item for k in ['title', 'reason']):
                        print(f"ğŸ”§ è‡ªå‹•æª¢æ¸¬åˆ°é™£åˆ—ï¼š{key}")
                        return {'selections': value}
        
        return data

# æŠ“å– RSS XML
def fetch_rss(date_str):
    url = f"https://japan-news-get.netlify.app/rss?date={date_str}"
    res = requests.get(url)
    if res.status_code != 200:
        raise Exception(f"RSS éŒ¯èª¤ï¼š{res.status_code}")
    return res.text

# è§£æ XML æ¨™é¡Œ
def parse_rss_titles(rss_xml):
    root = ET.fromstring(rss_xml)
    items = root.findall(".//item")
    titles = []
    for item in items:
        title_element = item.find("title")
        if title_element is not None and title_element.text:
            title = title_element.text.strip()
            if any(skip in title for skip in ["Yahoo Japan", "åœ°éœ‡æƒ…å ±"]):
                continue
            titles.append(title)
    return titles

# å‘¼å« GPT ä¸¦è§£æ - ç°¡åŒ–ç‰ˆ
def call_gpt_format_selection(titles: List[str]) -> HeadlineSelection:
    prompt = f"""
ä½ æ˜¯å°ç£çš„åœ‹éš›æ–°èç·¨è¼¯ï¼Œä»¥ä¸‹æ˜¯æ—¥æœ¬æ–°èæ¨™é¡Œï¼Œè«‹å¾ä¸­é¸å‡º 5 å‰‡æ–°èï¼Œä¸¦èªªæ˜é¸æ“‡ç†ç”±èˆ‡å»ºè­°æ’°å¯«è§’åº¦ã€‚

**é‡è¦æŒ‡ç¤ºï¼šç„¡è«–å¦‚ä½•éƒ½å¿…é ˆé¸å‡ºæ­£å¥½ 5 å‰‡æ–°èï¼Œå³ä½¿æ¨™é¡Œçœ‹èµ·ä¾†ä¸å¤ æœ‰è¶£æˆ–ä¸å®Œå…¨ç¬¦åˆæ¢ä»¶ï¼Œä¹Ÿè¦å¾ç¾æœ‰æ¨™é¡Œä¸­é¸å‡ºæœ€ç›¸é—œçš„ 5 å‰‡ã€‚**

å„ªå…ˆæ¢ä»¶ï¼ˆç›¡é‡ç¬¦åˆï¼Œä½†ä¸æ˜¯å¿…é ˆï¼‰ï¼š
1. æœ‰åŠ©å°ç£ç†è§£æ—¥æœ¬æ”¿æ²»ã€å¤–äº¤ã€ç¶“æ¿Ÿã€æ–‡åŒ–
2. èƒ½ä½œç‚ºå°ä¸­æ”¿ç­–æˆ–å€åŸŸå®‰å…¨åƒè€ƒ

å¦‚æœç¬¦åˆä¸Šè¿°æ¢ä»¶çš„æ–°èä¸è¶³ 5 å‰‡ï¼Œè«‹æŒ‰ä»¥ä¸‹å„ªå…ˆé †åºè£œè¶³ï¼š
1. æ—¥æœ¬æ”¿æ²»ã€ç¶“æ¿Ÿã€ç¤¾æœƒé‡è¦äº‹ä»¶
2. æ—¥æœ¬åœ‹éš›é—œä¿‚æˆ–å¤–äº¤å‹•æ…‹
3. æ—¥æœ¬ç§‘æŠ€ã€ç”¢æ¥­ç™¼å±•
4. ä»»ä½•å…·æœ‰æ–°èåƒ¹å€¼çš„æ—¥æœ¬ç›¸é—œæ–°è

**å¼·åˆ¶è¦æ±‚ï¼š**
- å¿…é ˆé¸å‡ºæ­£å¥½ 5 å‰‡æ–°èï¼Œä¸å¯ä»¥é¸å°‘æ–¼ 5 å‰‡
- å³ä½¿æ¨™é¡Œè³ªé‡ä¸ç†æƒ³ï¼Œä¹Ÿè¦å¾çµ¦å®šçš„æ¨™é¡Œä¸­é¸å‡ºæœ€å¥½çš„ 5 å‰‡
- ä¸å¯ä»¥å›å‚³ç©ºé™£åˆ—æˆ–å°‘æ–¼ 5 å€‹é …ç›®çš„é™£åˆ—

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å‚³ï¼Œå‹™å¿…åŒ…å« 5 å‰‡æ–°èï¼š
{{
  "selections": [
    {{
      "title": "æ–°èæ¨™é¡Œ",
      "reason": "é¸æ“‡ç†ç”±", 
      "writing_direction": "å»ºè­°æ’°å¯«è§’åº¦"
    }},
    {{
      "title": "æ–°èæ¨™é¡Œ2",
      "reason": "é¸æ“‡ç†ç”±2", 
      "writing_direction": "å»ºè­°æ’°å¯«è§’åº¦2"
    }},
    {{
      "title": "æ–°èæ¨™é¡Œ3",
      "reason": "é¸æ“‡ç†ç”±3", 
      "writing_direction": "å»ºè­°æ’°å¯«è§’åº¦3"
    }},
    {{
      "title": "æ–°èæ¨™é¡Œ4",
      "reason": "é¸æ“‡ç†ç”±4", 
      "writing_direction": "å»ºè­°æ’°å¯«è§’åº¦4"
    }},
    {{
      "title": "æ–°èæ¨™é¡Œ5",
      "reason": "é¸æ“‡ç†ç”±5", 
      "writing_direction": "å»ºè­°æ’°å¯«è§’åº¦5"
    }}
  ]
}}

**å†æ¬¡å¼·èª¿ï¼šé™£åˆ—ä¸­å¿…é ˆæœ‰æ­£å¥½ 5 å€‹æ–°èç‰©ä»¶ï¼Œçµ•å°ä¸å¯ä»¥æ˜¯ç©ºé™£åˆ—æˆ–å°‘æ–¼ 5 å€‹é …ç›®ã€‚**
"""
    
    # é™åˆ¶æ¨™é¡Œæ•¸é‡é¿å… token éå¤š
    limited_titles = titles[:100]
    
    print(f"ğŸ“ ç™¼é€çµ¦ GPT çš„æ¨™é¡Œæ•¸é‡ï¼š{len(limited_titles)}")
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„æ–°èç·¨è¼¯ã€‚**æœ€é‡è¦çš„è¦å‰‡ï¼šç„¡è«–å¦‚ä½•éƒ½å¿…é ˆé¸å‡ºæ­£å¥½5å‰‡æ–°èï¼Œå³ä½¿æ¨™é¡Œè³ªé‡ä¸ç†æƒ³ä¹Ÿè¦é¸å‡ºæœ€å¥½çš„5å‰‡ã€‚çµ•å°ä¸å¯ä»¥å›å‚³å°‘æ–¼5å€‹é …ç›®çš„é™£åˆ—ã€‚** è«‹åš´æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼å›å‚³çµæœã€‚"},
        {"role": "user", "content": prompt + "\n\næ–°èæ¨™é¡Œï¼š\n" + "\n".join([f"- {title}" for title in limited_titles])}
    ]
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            response_format={"type": "json_object"},
            temperature=0.3
        )
        
        # ç›´æ¥ä½¿ç”¨ model_validate_jsonï¼Œè®“ Pydantic è™•ç†æ ¼å¼è½‰æ›
        parsed = HeadlineSelection.model_validate_json(response.choices[0].message.content)
        print(f"âœ… GPT åˆ†ææˆåŠŸï¼Œé¸å‡º {len(parsed.selections)} å‰‡æ–°è")
        
        return parsed
        
    except Exception as e:
        print(f"âŒ GPT åˆ†æå¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        raise

# æ”¹é€²çš„å„²å­˜å‡½æ•¸
def save_to_supabase(date_str: str, selection: HeadlineSelection):
    print(f"\nğŸ“Š æº–å‚™å„²å­˜ {len(selection.selections)} å‰‡é¸ä¸­çš„æ–°èåˆ° Supabase")
    print(f"ğŸ“… æ—¥æœŸï¼š{date_str}")
    print(f"ğŸ—„ï¸ è¡¨æ ¼ï¼šselected_news")
    print("-" * 50)
    
    success_count = 0
    error_count = 0
    
    for i, item in enumerate(selection.selections, 1):
        try:
            data = {
                "id": str(uuid4()),
                "date": date_str,
                "title": item.title,
                "reason": item.reason,
                "writing_direction": item.writing_direction,
                "created_at": datetime.now(timezone.utc).isoformat()
            }
            
            print(f"\nğŸ“ ç¬¬ {i} å‰‡æ–°èï¼š")
            print(f"   æ¨™é¡Œï¼š{item.title[:60]}{'...' if len(item.title) > 60 else ''}")
            print(f"   ç†ç”±ï¼š{item.reason[:60]}{'...' if len(item.reason) > 60 else ''}")
            print(f"   æ–¹å‘ï¼š{item.writing_direction[:60]}{'...' if len(item.writing_direction) > 60 else ''}")
            
            # åŸ·è¡Œæ’å…¥
            res = supabase.table("selected_news").insert(data).execute()
            
            # Supabase æˆåŠŸæ’å…¥æ™‚æª¢æŸ¥ data æ˜¯å¦å­˜åœ¨
            if hasattr(res, 'data') and res.data:
                print(f"   âœ… å„²å­˜æˆåŠŸï¼ID: {data['id'][:8]}...")
                success_count += 1
            else:
                print(f"   âš ï¸ å„²å­˜å¯èƒ½å¤±æ•—ï¼Œå›æ‡‰ï¼š{res}")
                error_count += 1
                
        except Exception as e:
            print(f"   âŒ å„²å­˜å¤±æ•—ï¼š{e}")
            error_count += 1
    
    print("-" * 50)
    print(f"ğŸ“ˆ å„²å­˜çµæœï¼šæˆåŠŸ {success_count} å‰‡ï¼Œå¤±æ•— {error_count} å‰‡")
    
    if success_count > 0:
        print(f"ğŸ‰ è³‡æ–™å·²å„²å­˜åˆ° Supabase è¡¨æ ¼ 'selected_news'")
        print(f"ğŸ“… å¯ä»¥åœ¨è³‡æ–™åº«ä¸­æŸ¥è©¢æ—¥æœŸ '{date_str}' çš„è¨˜éŒ„")

# æŸ¥è©¢è³‡æ–™åº«å‡½æ•¸
def check_database(date_str=None):
    """æª¢æŸ¥è³‡æ–™åº«ä¸­å„²å­˜çš„è³‡æ–™"""
    print("\nğŸ” æª¢æŸ¥è³‡æ–™åº«ä¸­çš„è³‡æ–™...")
    
    try:
        if date_str:
            # æŸ¥è©¢ç‰¹å®šæ—¥æœŸ
            res = supabase.table("selected_news").select("*").eq("date", date_str).execute()
            print(f"ğŸ“… æŸ¥è©¢æ—¥æœŸï¼š{date_str}")
        else:
            # æŸ¥è©¢æœ€è¿‘çš„è³‡æ–™
            res = supabase.table("selected_news").select("*").order("created_at", desc=True).limit(10).execute()
            print("ğŸ“… æŸ¥è©¢æœ€è¿‘ 10 ç­†è³‡æ–™")
        
        if res.data:
            print(f"âœ… æ‰¾åˆ° {len(res.data)} ç­†è¨˜éŒ„ï¼š")
            for i, record in enumerate(res.data, 1):
                print(f"\n{i}. ID: {record.get('id', 'N/A')[:8]}...")
                print(f"   æ—¥æœŸ: {record.get('date', 'N/A')}")
                print(f"   æ¨™é¡Œ: {record.get('title', 'N/A')[:50]}...")
                print(f"   ç†ç”±: {record.get('reason', 'N/A')[:50]}...")
                print(f"   å»ºç«‹æ™‚é–“: {record.get('created_at', 'N/A')}")
        else:
            print("âŒ è³‡æ–™åº«ä¸­æ²’æœ‰æ‰¾åˆ°è³‡æ–™")
            
    except Exception as e:
        print(f"âŒ æŸ¥è©¢è³‡æ–™åº«å¤±æ•—ï¼š{e}")

# ä¸»æµç¨‹
def main():
    JST = timezone(timedelta(hours=9))
    now = datetime.now(JST)
    hour_now = now.hour

    print(f"ğŸ• ç•¶å‰æ™‚é–“ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')} JST")
    
    if hour_now < 15:
        target_dates = [
            (now - timedelta(days=2)).strftime('%Y%m%d'),
            (now - timedelta(days=1)).strftime('%Y%m%d')
        ]
        print("ğŸŒ… æ—©æ–¼ä¸‹åˆ3é»ï¼Œåˆ†æå‰å…©å¤©çš„æ–°è")
    else:
        target_dates = [
            (now - timedelta(days=1)).strftime('%Y%m%d'),
            now.strftime('%Y%m%d')
        ]
        print("ğŸŒ† ä¸‹åˆ3é»å¾Œï¼Œåˆ†ææ˜¨å¤©å’Œä»Šå¤©çš„æ–°è")
    
    print(f"ğŸ“‹ ç›®æ¨™æ—¥æœŸï¼š{target_dates}")

    all_titles = []
    latest_date = target_dates[-1]

    for date_str in target_dates:
        try:
            print(f"\nğŸ“¡ æŠ“å– RSSï¼š{date_str}")
            rss_xml = fetch_rss(date_str)
            titles = parse_rss_titles(rss_xml)
            all_titles.extend(titles)
            print(f"   âœ… å–å¾— {len(titles)} å‰‡æ¨™é¡Œ")
        except Exception as e:
            print(f"   âš ï¸ RSS {date_str} æŠ“å–å¤±æ•—ï¼š{e}")

    if not all_titles:
        print("âŒ ç„¡æ–°èæ¨™é¡Œå¯åˆ†æ")
        return

    # å»é‡è¤‡
    unique_titles = list(dict.fromkeys(all_titles))
    print(f"\nğŸ§  é–‹å§‹åˆ†æï¼Œå»é‡å¾Œå…± {len(unique_titles)} å‰‡")
    print(f"ğŸ“Š å°‡é¸å‡º 5 å‰‡é‡è¦æ–°è")
    
    try:
        result = call_gpt_format_selection(unique_titles)
        print(f"\nâœ… GPT åˆ†æå®Œæˆï¼é¸å‡º {len(result.selections)} å‰‡æ–°è")
        
        # é¡¯ç¤ºé¸ä¸­çš„æ–°èåˆ—è¡¨
        print("\nğŸ“‹ é¸ä¸­çš„æ–°èï¼š")
        for i, item in enumerate(result.selections, 1):
            print(f"{i}. {item.title}")
        
        # å„²å­˜åˆ°è³‡æ–™åº«
        save_to_supabase(latest_date, result)
        
        # åŸ·è¡Œå®Œç•¢å¾Œæª¢æŸ¥è³‡æ–™åº«
        print("\n" + "="*60)
        check_database(latest_date)
        
    except Exception as e:
        print(f"âŒ GPT æˆ–å„²å­˜éšæ®µéŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()