import os
from dotenv import load_dotenv

# è¼‰å…¥ .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
load_dotenv()
import sys
import subprocess
import requests
import xml.etree.ElementTree as ET
import json
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client

# ğŸ› ï¸ è‡ªå‹•å®‰è£æ‰€éœ€å¥—ä»¶
def ensure_package(pkg):
    try:
        __import__(pkg)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])

for p in ['ollama', 'supabase']:
    ensure_package(p)

import ollama
from supabase import create_client

# ğŸ§  ç¢ºä¿ llama4 æ¨¡å‹å­˜åœ¨
def ensure_llama_model():
    result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
    if "llama4:128x17b" not in result.stdout:
        print("â¬‡ï¸ Pull llama4:128x17b...")
        subprocess.run(["ollama", "pull", "llama4:128x17b"], check=True)

# ğŸ” Supabase åˆå§‹åŒ–
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ğŸŒ æŠ“å– RSS
def fetch_rss(date_str):
    url = f"https://japan-news-get.netlify.app/rss?date={date_str}"
    res = requests.get(url)
    res.raise_for_status()
    return res.text

def parse_titles(rss_xml):
    root = ET.fromstring(rss_xml)
    items = root.findall(".//item")
    titles = []
    for item in items:
        title = item.find("title").text.strip()
        if any(skip in title for skip in ["Yahoo Japan ãƒˆãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹", "åœ°éœ‡æƒ…å ±"]):
            continue
        titles.append(title)
    return titles

# ğŸ¤– å‘¼å« LLM
def analyze_titles(titles):
    prompt = f"""
ä½ æ˜¯ä¸€ä½ç‚ºè‡ºç£è£½ä½œåœ‹éš›æ–°èçš„æ—¥æœ¬è§€å¯Ÿç«™æ–°èç·¨è¼¯ã€‚è«‹å¾ä¸‹åˆ—æ—¥æœ¬æ–°èæ¨™é¡Œä¸­é¸å‡ºæœ€å€¼å¾—å ±å°çš„5å‰‡ï¼š

æ¨™é¡Œå¦‚ä¸‹ï¼š
{json.dumps(titles, ensure_ascii=False, indent=2)}

é¸æ“‡æ¨™æº–å¦‚ä¸‹ï¼š
- å¢é€²å°ç£äººå°æ—¥æœ¬çš„æ”¿æ²»ã€ç¶“æ¿Ÿã€å¤–äº¤èˆ‡æ–‡åŒ–ç†è§£
- å°æ–¼ç†è§£ç¾ä¸­å°é—œä¿‚æœ‰åƒè€ƒåƒ¹å€¼
- åœ¨å¯èƒ½çš„å°æµ·è¡çªä¸­ï¼Œæœ‰åŠ©æ–¼å»ºç«‹å°æ—¥èªçŸ¥èˆ‡å› æ‡‰ç­–ç•¥

è«‹è¼¸å‡º JSON æ ¼å¼ï¼ŒåŒ…å«ï¼š
1. é¸å‡ºçš„5å€‹æ¨™é¡Œ
2. ç‚ºä½•é¸æ“‡ï¼ˆæ¯å€‹ç°¡è¦èªªæ˜ï¼‰
3. æ¯å‰‡å»ºè­°çš„å¯«ä½œæ–¹å‘èˆ‡è§’åº¦
"""
    res = ollama.chat(
        model="llama4:128x17b",
        messages=[{"role": "user", "content": prompt}]
    )
    return res["message"]["content"]

# ğŸ’¾ å¯«å…¥ Supabase
def store_to_supabase(date_str, source, result):
    supabase.table("rss_selection_log").insert({
        "date": date_str,
        "source": source,
        "result": json.loads(result),
        "fetched_at": datetime.utcnow().isoformat()
    }).execute()

# ğŸ“¤ ç™¼é€ Webhook
def send_webhook(result_json):
    webhook_url = os.getenv("WEBHOOK_URL")
    if not webhook_url:
        print("âš ï¸ æœªè¨­å®š WEBHOOK_URL")
        return
    try:
        requests.post(webhook_url, json=result_json)
        print("âœ… Webhook å·²é€å‡º")
    except Exception as e:
        print("âŒ Webhook ç™¼é€éŒ¯èª¤ï¼š", str(e))

# ğŸ§­ ä¸»æµç¨‹
def main():
    ensure_llama_model()
    JST = timezone(timedelta(hours=9))
    today = datetime.now(JST).strftime('%Y%m%d')
    yesterday = (datetime.now(JST) - timedelta(days=1)).strftime('%Y%m%d')

    titles = []
    for d in [yesterday, today]:
        try:
            xml = fetch_rss(d)
            titles += parse_titles(xml)
        except Exception as e:
            print(f"âš ï¸ RSS {d} è®€å–å¤±æ•—ï¼š{e}")

    if not titles:
        print("âŒ æ²’æœ‰å¯ç”¨æ¨™é¡Œ")
        return

    print("ğŸ¤– åˆ†æä¸­...")
    llm_result = analyze_titles(titles)

    print("ğŸ“¦ å„²å­˜ä¸­...")
    store_to_supabase(today, "rss", llm_result)

    print("ğŸ“¡ å‚³é€é€šçŸ¥...")
    send_webhook({
        "date": today,
        "source": "rss",
        "llm_result": json.loads(llm_result)
    })

if __name__ == "__main__":
    main()
