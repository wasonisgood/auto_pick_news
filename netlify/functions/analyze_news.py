# netlify/functions/analyze_news.py
import json
import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from typing import List, Any
from uuid import uuid4

import requests
from pydantic import BaseModel, model_validator
from openai import OpenAI
from supabase import create_client, Client

# åˆå§‹åŒ–å®¢æˆ¶ç«¯
def get_clients():
    """åˆå§‹åŒ– OpenAI å’Œ Supabase å®¢æˆ¶ç«¯"""
    openai_client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    supabase_client = create_client(
        os.environ["SUPABASE_URL"], 
        os.environ["SUPABASE_KEY"]
    )
    return openai_client, supabase_client

# Pydantic æ¨¡åž‹
class SelectedHeadline(BaseModel):
    title: str
    reason: str
    writing_direction: str

class HeadlineSelection(BaseModel):
    selections: List[SelectedHeadline]
    
    @model_validator(mode='before')
    @classmethod
    def extract_selections(cls, data: Any) -> Any:
        """è‡ªå‹•å¾žä¸åŒçš„éµåä¸­æå–é¸é …é™£åˆ—"""
        if isinstance(data, dict):
            possible_keys = [
                'selections', 'selected_articles', 'articles',
                'news', 'selected_news', 'items', 'results'
            ]
            
            for key in possible_keys:
                if key in data and isinstance(data[key], list):
                    selections_data = data[key]
                    
                    if len(selections_data) == 0:
                        raise ValueError('é¸é …é™£åˆ—ä¸èƒ½ç‚ºç©º')
                    
                    cleaned_selections = []
                    for i, item in enumerate(selections_data):
                        if isinstance(item, dict):
                            cleaned_item = {
                                'title': item.get('title', f'æ–°èž {i+1}'),
                                'reason': item.get('reason', 'æœªæä¾›ç†ç”±'),
                                'writing_direction': item.get('writing_direction', 'æœªæä¾›å»ºè­°')
                            }
                            cleaned_selections.append(cleaned_item)
                    
                    return {'selections': cleaned_selections}
        
        return data

def fetch_rss(date_str):
    """æŠ“å– RSS XML"""
    url = f"https://japan-news-get.netlify.app/rss?date={date_str}"
    response = requests.get(url, timeout=30)
    if response.status_code != 200:
        raise Exception(f"RSS éŒ¯èª¤ï¼š{response.status_code}")
    return response.text

def parse_rss_titles(rss_xml):
    """è§£æž XML æ¨™é¡Œ"""
    root = ET.fromstring(rss_xml)
    items = root.findall(".//item")
    titles = []
    for item in items:
        title_element = item.find("title")
        if title_element is not None and title_element.text:
            title = title_element.text.strip()
            if any(skip in title for skip in ["Yahoo Japan", "åœ°éœ‡æƒ…å ±"]):
                continue
            titles.append(title)
    return titles

def analyze_with_gpt(titles: List[str], openai_client) -> HeadlineSelection:
    """ä½¿ç”¨ GPT åˆ†æžæ–°èž"""
    prompt = """
ä½ æ˜¯å°ç£çš„åœ‹éš›æ–°èžç·¨è¼¯ï¼Œä»¥ä¸‹æ˜¯æ—¥æœ¬æ–°èžæ¨™é¡Œï¼Œè«‹å¾žä¸­é¸å‡º 5 å‰‡æ–°èžï¼Œä¸¦èªªæ˜Žé¸æ“‡ç†ç”±èˆ‡å»ºè­°æ’°å¯«è§’åº¦ã€‚

**é‡è¦æŒ‡ç¤ºï¼šç„¡è«–å¦‚ä½•éƒ½å¿…é ˆé¸å‡ºæ­£å¥½ 5 å‰‡æ–°èžï¼Œå³ä½¿æ¨™é¡Œçœ‹èµ·ä¾†ä¸å¤ æœ‰è¶£æˆ–ä¸å®Œå…¨ç¬¦åˆæ¢ä»¶ï¼Œä¹Ÿè¦å¾žç¾æœ‰æ¨™é¡Œä¸­é¸å‡ºæœ€ç›¸é—œçš„ 5 å‰‡ã€‚**

å„ªå…ˆæ¢ä»¶ï¼ˆç›¡é‡ç¬¦åˆï¼Œä½†ä¸æ˜¯å¿…é ˆï¼‰ï¼š
1. æœ‰åŠ©å°ç£ç†è§£æ—¥æœ¬æ”¿æ²»ã€å¤–äº¤ã€ç¶“æ¿Ÿã€æ–‡åŒ–
2. èƒ½ä½œç‚ºå°ä¸­æ”¿ç­–æˆ–å€åŸŸå®‰å…¨åƒè€ƒ

å¦‚æžœç¬¦åˆä¸Šè¿°æ¢ä»¶çš„æ–°èžä¸è¶³ 5 å‰‡ï¼Œè«‹æŒ‰ä»¥ä¸‹å„ªå…ˆé †åºè£œè¶³ï¼š
1. æ—¥æœ¬æ”¿æ²»ã€ç¶“æ¿Ÿã€ç¤¾æœƒé‡è¦äº‹ä»¶
2. æ—¥æœ¬åœ‹éš›é—œä¿‚æˆ–å¤–äº¤å‹•æ…‹
3. æ—¥æœ¬ç§‘æŠ€ã€ç”¢æ¥­ç™¼å±•
4. ä»»ä½•å…·æœ‰æ–°èžåƒ¹å€¼çš„æ—¥æœ¬ç›¸é—œæ–°èž

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›žå‚³ï¼Œå‹™å¿…åŒ…å« 5 å‰‡æ–°èžï¼š
{
  "selections": [
    {
      "title": "æ–°èžæ¨™é¡Œ",
      "reason": "é¸æ“‡ç†ç”±", 
      "writing_direction": "å»ºè­°æ’°å¯«è§’åº¦"
    }
  ]
}

**å¼·åˆ¶è¦æ±‚ï¼šé™£åˆ—ä¸­å¿…é ˆæœ‰æ­£å¥½ 5 å€‹æ–°èžç‰©ä»¶ï¼Œçµ•å°ä¸å¯ä»¥æ˜¯ç©ºé™£åˆ—æˆ–å°‘æ–¼ 5 å€‹é …ç›®ã€‚**
"""
    
    # é™åˆ¶æ¨™é¡Œæ•¸é‡é¿å… token éŽå¤š
    limited_titles = titles[:50]
    
    messages = [
        {
            "role": "system", 
            "content": "ä½ æ˜¯å°ˆæ¥­çš„æ–°èžç·¨è¼¯ã€‚æœ€é‡è¦çš„è¦å‰‡ï¼šç„¡è«–å¦‚ä½•éƒ½å¿…é ˆé¸å‡ºæ­£å¥½5å‰‡æ–°èžã€‚è«‹åš´æ ¼æŒ‰ç…§JSONæ ¼å¼å›žå‚³çµæžœã€‚"
        },
        {
            "role": "user", 
            "content": prompt + "\n\næ–°èžæ¨™é¡Œï¼š\n" + "\n".join([f"- {title}" for title in limited_titles])
        }
    ]
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        response_format={"type": "json_object"},
        temperature=0.3
    )
    
    parsed = HeadlineSelection.model_validate_json(response.choices[0].message.content)
    return parsed

def save_to_database(date_str: str, selection: HeadlineSelection, supabase_client):
    """å„²å­˜åˆ° Supabase è³‡æ–™åº«"""
    success_count = 0
    errors = []
    
    for item in selection.selections:
        try:
            data = {
                "id": str(uuid4()),
                "date": date_str,
                "title": item.title,
                "reason": item.reason,
                "writing_direction": item.writing_direction,
                "created_at": datetime.now(timezone.utc).isoformat()
            }
            
            res = supabase_client.table("selected_news").insert(data).execute()
            
            if hasattr(res, 'data') and res.data:
                success_count += 1
            else:
                errors.append(f"å„²å­˜å¤±æ•—ï¼š{item.title[:30]}...")
                
        except Exception as e:
            errors.append(f"éŒ¯èª¤ï¼š{str(e)}")
    
    return success_count, errors

def handler(event, context):
    """Netlify Function ä¸»è™•ç†å‡½æ•¸"""
    
    # è¨˜éŒ„åŸ·è¡Œé–‹å§‹
    start_time = datetime.now(timezone.utc)
    log_messages = []
    
    try:
        # åˆå§‹åŒ–å®¢æˆ¶ç«¯
        openai_client, supabase_client = get_clients()
        log_messages.append("âœ… å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # è¨ˆç®—ç›®æ¨™æ—¥æœŸï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰
        JST = timezone(timedelta(hours=9))
        now_jst = datetime.now(JST)
        
        # ç¸½æ˜¯åˆ†æžå‰ä¸€å¤©çš„æ–°èžï¼ˆå› ç‚ºå‡Œæ™¨3é»žåŸ·è¡Œï¼‰
        target_date = (now_jst - timedelta(days=1)).strftime('%Y%m%d')
        log_messages.append(f"ðŸ“… ç›®æ¨™åˆ†æžæ—¥æœŸï¼š{target_date}")
        
        # æŠ“å–æ–°èž
        log_messages.append(f"ðŸ“¡ é–‹å§‹æŠ“å– {target_date} çš„æ–°èž...")
        rss_xml = fetch_rss(target_date)
        titles = parse_rss_titles(rss_xml)
        
        if not titles:
            raise Exception("æœªå–å¾—ä»»ä½•æ–°èžæ¨™é¡Œ")
        
        # åŽ»é‡è¤‡
        unique_titles = list(dict.fromkeys(titles))
        log_messages.append(f"ðŸ“° å–å¾— {len(titles)} å‰‡æ¨™é¡Œï¼ŒåŽ»é‡å¾Œ {len(unique_titles)} å‰‡")
        
        # GPT åˆ†æž
        log_messages.append("ðŸ§  é–‹å§‹ GPT åˆ†æž...")
        selection = analyze_with_gpt(unique_titles, openai_client)
        log_messages.append(f"âœ… GPT åˆ†æžå®Œæˆï¼Œé¸å‡º {len(selection.selections)} å‰‡æ–°èž")
        
        # å„²å­˜åˆ°è³‡æ–™åº«
        log_messages.append("ðŸ’¾ é–‹å§‹å„²å­˜åˆ°è³‡æ–™åº«...")
        success_count, errors = save_to_database(target_date, selection, supabase_client)
        
        # æº–å‚™å›žæ‡‰
        execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()
        
        result = {
            "success": True,
            "message": f"æˆåŠŸåˆ†æžä¸¦å„²å­˜ {success_count} å‰‡æ–°èž",
            "data": {
                "date": target_date,
                "total_titles": len(titles),
                "unique_titles": len(unique_titles),
                "selected_count": len(selection.selections),
                "saved_count": success_count,
                "selected_news": [
                    {
                        "title": item.title,
                        "reason": item.reason[:100] + "..." if len(item.reason) > 100 else item.reason,
                        "writing_direction": item.writing_direction[:100] + "..." if len(item.writing_direction) > 100 else item.writing_direction
                    }
                    for item in selection.selections
                ],
                "execution_time_seconds": round(execution_time, 2)
            },
            "logs": log_messages,
            "errors": errors if errors else None,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        return {
            "statusCode": 200,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*"
            },
            "body": json.dumps(result, ensure_ascii=False, indent=2)
        }
        
    except Exception as e:
        execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()
        
        error_result = {
            "success": False,
            "message": f"åŸ·è¡Œå¤±æ•—ï¼š{str(e)}",
            "logs": log_messages,
            "execution_time_seconds": round(execution_time, 2),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        return {
            "statusCode": 500,
            "headers": {
                "Content-Type": "application/json",
                "Access-Control-Allow-Origin": "*"
            },
            "body": json.dumps(error_result, ensure_ascii=False, indent=2)
        }

# ç‚ºäº†æœ¬åœ°æ¸¬è©¦
if __name__ == "__main__":
    # æ¨¡æ“¬ Netlify event å’Œ context
    test_event = {}
    test_context = {}
    
    result = handler(test_event, test_context)
    print(json.dumps(json.loads(result["body"]), ensure_ascii=False, indent=2))